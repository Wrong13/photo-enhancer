# Вводная


**Повышение качества фото с помощью автокодировщика**

Работа выполнена студентом СибАДИ г.Омска

## Краткое описание проблемы
Проблема супер-разрешения (Super-Resolution, SR) заключается в восстановлении изображений высокого разрешения (HR) из низкого разрешения (LR). Это важно для медицины, спутниковой съемки, видеонаблюдения и других областей.

## Основные концепции
- **LR и HR изображения**: LR – уменьшенные версии исходных изображений, HR – оригинальные.
- **SR-сети**: Используются сверточные нейронные сети, такие как SRCNN, SRResNet, EDSR.
- **Перцептуальные потери**: Потери, основанные на признаках, а не только на пикселях, улучшают визуальное качество.
- **CBAM (Convolutional Block Attention Module)**: Добавляет внимание по каналам и пространству, усиливая значимые признаки.

## Схема архитектуры
- Вход: LR изображение
- Первичный свёрточный слой + PReLU
- Последовательность **Residual/CBAM блоков**
- Сумма с исходным выходом после остаточного блока
- Upsample через PixelShuffle
- Конечный свёрточный слой -> выход SR изображения

# Теория

## Существующие архитектуры

## 1. CNN-базированные модели (свёрточные сети)

Используют **свёрточные блоки** для восстановления деталей.

- **SRCNN (Super-Resolution CNN)**
  - Архитектура: 3 свёрточных слоя, прямое увеличение разрешения.
  - ➕ Простота, быстрый старт.  
  - ➖ Мало деталей, устаревшая для больших масштабов.

- **FSRCNN (Fast SRCNN)**
  - Улучшенная версия SRCNN с ускоренной обработкой.

- **VDSR (Very Deep Super Resolution)**
  - ~20 слоёв, использует остаточные соединения.
  - Обеспечивает более высокое качество восстановления.

---

## 2. Residual Networks (ResNet) и остаточные блоки

Используются для **глубоких сетей** без проблем затухающих градиентов.

- **EDSR (Enhanced Deep Super-Resolution Network)**
  - Построена на ResNet-блоках без нормализации.  
  - Отлично работает для ×2 и ×4.

- **RCAN (Residual Channel Attention Network)**
  - Добавляет **attention по каналам**, чтобы сеть фокусировалась на важных признаках.  
  - Хорошо работает на изображениях с текстурами.

---

## 3. GAN-базированные подходы (Generative Adversarial Networks)

Используют **генератор + дискриминатор** для фотореалистичных деталей.

- **SRGAN (Super-Resolution GAN)**
  - Генератор создаёт изображение высокого разрешения, дискриминатор оценивает реалистичность.  
  - Отлично восстанавливает текстуры, делает картинку «живой».

- **ESRGAN (Enhanced SRGAN)**
  - Улучшенная версия с **Residual-in-Residual Dense Block (RRDB)**.  
  - Одно из лучших решений для фото с деталями.

- **Real-ESRGAN**
  - Обучен на **реальных изображениях с шумом и артефактами**.  
  - Работает с фотографиями и сканами.

---

## 4. Transformer-базированные модели

Используют **attention** для глобального контекста.

- **SwinIR (Swin Transformer for Image Restoration)**
  - Основан на **Swin Transformer**.  
  - Универсален: супер-разрешение, шумоподавление, восстановление.

- **HAT (Hierarchical Attention Transformer)**
  - Использует **иерархическое внимание**.  
  - Ещё лучше на сложных текстурах и деталях.

---

## 5. Другие подходы

- **Denoising + SR**
  - Сначала подавление шума (например, **DnCNN, CBDNet**), затем увеличение разрешения.

- **DBPN (Deep Back-Projection Networks)**
  - Использует многократное проецирование между LR ↔ HR.  
  - Даёт отличную детализацию.


## Выбранная модель: SRResNet

- **Архитектура**  
  Основана на **Residual Blocks**, как ResNet, но адаптирована под супер-разрешение.  
  Включает: несколько `ResBlock` → **апскейлинг (PixelShuffle)** → выходное изображение.

- **Преимущества**  
  - Стабильное обучение.  
  - Хорошее восстановление текстур без артефактов.  
  - Более «чистое» изображение по сравнению с GAN-моделями.  
  - Проще в реализации и обучении, чем EDSR/RCAN.

- **Недостатки**  
  - Не генерирует полностью «фотореалистичные» детали (картинка иногда выглядит «плоской»).  
  - Восстанавливает хуже мелкие детали, чем ESRGAN/Real-ESRGAN.  

- **Почему выбрана**  
  SRResNet занимает **середину между простыми CNN и GAN-подходами**:  
  - Легче, чем сложные трансформеры (SwinIR, HAT).  
  - Стабильнее, чем GAN (нет риска артефактов).  
  - Даёт баланс между **качеством, стабильностью и сложностью реализации**.

![image.png](https://neurohive.io/wp-content/uploads/2019/01/resnet-570x328.png)

Архитектура простой ResNet
